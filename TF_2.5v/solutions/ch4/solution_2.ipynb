{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Q2.</span>\n",
    "<b>Fashion-MNIST 문제에서 과대적합에 대해 설명했습니다.  \n",
    "<span style=\"color:red\">데이터가 충분하지 않다면, 모델의 깊이가 깊은 경우 과대적합 문제에 노출될 확률이 매우 높은데요.</span>\n",
    "\n",
    "필자는 이를 어느정도 조절하기 위해 매우 깊은 Dense층을 사용하지 않은 상태로 여러분들에게 예제 코드를 제공했습니다.\n",
    "\n",
    "Fashion-MNIST에서 총 세 가지의 실험을 진행해보았으면 좋겠습니다.  \n",
    "각 실험에서 모델의 수렴 속도와 과대적합이 어느 구간에서 발생하는지 확인할 수 있어야 합니다.\n",
    "\n",
    "끝까지 학습시켜보고, 일어나는 변화를 관찰하세요.</b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "\n",
    "# Fashion-MNIST 데이터를 다운받습니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# 값의 범위를 0 ~ 1로 만들어줍니다.\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 데이터의 레이블을 범주형 형태로 변경합니다.\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# 검증 데이터 세트를 만듭니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/테스트 데이터를 0.7/0.3의 비율로 분리합니다.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.3, random_state = 777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 매우 적은 개수의 은닉 유닛과 적은 개수의 Dense층을 통한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "small_model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "small_model.add(Flatten(input_shape = (28, 28))) # (28, 28) -> .(28 * 28)\n",
    "small_model.add(Dense(10, activation = 'relu'))\n",
    "small_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "small_model.compile(optimizer='adam', # 옵티마이저 : Adam\n",
    "              loss = 'categorical_crossentropy', # 손실 함수 : categorical_crossentropy\n",
    "              metrics=['acc']) # 모니터링 할 평가지표 : acc\n",
    "\n",
    "# loss, metrics, 학습 속도, 학습 여부 등 변화를 관찰하고, \n",
    "# 책의 예제 코드 결과와 비교해보세요.\n",
    "small_model.fit(x_train, y_train, \n",
    "                epochs = 30, \n",
    "                batch_size = 128, \n",
    "                validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 이 책의 예제코드를 통한 학습(책 코드 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 많은 개수의 은닉 유닛과 많은 개수의 Dense층을 통한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "big_model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 은닉 유닛 개수는 컴퓨팅 환경에 따라 조절하세요.\n",
    "# 너무 크면 Out Of Memory 에러가 발생할 수 있습니다.\n",
    "big_model.add(Flatten(input_shape = (28, 28))) # (28, 28) -> .(28 * 28)\n",
    "big_model.add(Dense(1024, activation = 'relu'))\n",
    "big_model.add(Dense(512, activation = 'relu'))\n",
    "big_model.add(Dense(256, activation = 'relu'))\n",
    "big_model.add(Dense(128, activation = 'relu'))\n",
    "big_model.add(Dense(64, activation = 'relu'))\n",
    "big_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "big_model.compile(optimizer='adam', # 옵티마이저 : Adam\n",
    "              loss = 'categorical_crossentropy', # 손실 함수 : categorical_crossentropy\n",
    "              metrics=['acc']) # 모니터링 할 평가지표 : acc\n",
    "\n",
    "# loss, metrics, 학습 속도, 학습 여부 등 변화를 관찰하고, \n",
    "# 책의 예제 코드 결과와 비교해보세요.\n",
    "# 과대적합 여부도 확인해보세요.\n",
    "# 과대적합은 train_loss는 감소하나, val_loss가 증가하는 경향을 보입니다.\n",
    "big_model.fit(x_train, y_train, \n",
    "                epochs = 30, \n",
    "                batch_size = 128, \n",
    "                validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 더 많은 개수의 은닉 유닛과 더 많은 개수의 Dense층을 통한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "big_model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 은닉 유닛 개수는 컴퓨팅 환경에 따라 조절하세요.\n",
    "# 너무 크면 Out Of Memory 에러가 발생할 수 있습니다.\n",
    "big_model.add(Flatten(input_shape = (28, 28))) # (28, 28) -> .(28 * 28)\n",
    "big_model.add(Dense(2048, activation = 'relu'))\n",
    "big_model.add(Dense(1024, activation = 'relu'))\n",
    "big_model.add(Dense(512, activation = 'relu'))\n",
    "big_model.add(Dense(256, activation = 'relu'))\n",
    "big_model.add(Dense(128, activation = 'relu'))\n",
    "big_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "big_model.compile(optimizer='adam', # 옵티마이저 : Adam\n",
    "              loss = 'categorical_crossentropy', # 손실 함수 : categorical_crossentropy\n",
    "              metrics=['acc']) # 모니터링 할 평가지표 : acc\n",
    "\n",
    "# loss, metrics, 학습 속도, 학습 여부 등 변화를 관찰하고, \n",
    "# 책의 예제 코드 결과와 비교해보세요.\n",
    "# 과대적합 여부도 확인해보세요.\n",
    "# 과대적합은 train_loss는 감소하나, val_loss가 증가하는 경향을 보입니다.\n",
    "big_model.fit(x_train, y_train, \n",
    "                epochs = 30, \n",
    "                batch_size = 128, \n",
    "                validation_data = (x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_study",
   "language": "python",
   "name": "keras_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
