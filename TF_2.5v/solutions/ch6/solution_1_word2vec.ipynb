{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Q1.</span>\n",
    "<b>우리는 IMDB 데이터셋에 Embedding층을 사용해보았습니다.  \n",
    "<span style=\"color:red\">Embedding층은 사전 학습된 가중치를 불러와 사용할 수 있습니다.</span>  \n",
    "\n",
    "여러 차례 언급했듯이, 사전 학습된 가중치를 사용한다는 것은 모델 성능에 큰 향상을 가져올지도 모릅니다.  \n",
    "사전 학습된 가중치를 불러온 Embedding층을 활용하여 문제를 해결해보기를 바랍니다.  \n",
    "밑의 힌트에서 언급한 Glove 또는 Word2Vec 중 하나를 선택하여 진행하세요.</b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Word2Vec 사용하기\n",
    "+ (저장소에선 파일을 제공하지 않습니다, 직접 다운받아주세요.) Word2Vec 다운로드  \n",
    "+ 다운로드 링크는 책에서 제공하고 있습니다.  \n",
    "<br>\n",
    "+ 압축을 풀면 GoogleNews-vectors-negative300.bin 파일이 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec.bin 파일 읽어오기\n",
    "+ binary 형태로 되어 있기 때문에 gensim 라이브러리를 활용하여,\n",
    "+ 읽어오도록 하겠습니다.\n",
    "+ 설치가 되어있지 않다면 설치 후 진행해주세요.\n",
    "+ pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Google word2vec 파일이 존재하는 경로를 입력하세요.\n",
    "word2vec = KeyedVectors.load_word2vec_format(os.path.join('your_word2vec_file_path', 'GoogleNews-vectors-negative300.bin'), \n",
    "                                             binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 확인할 수 있습니다.\n",
    "# for word in word2vec.vocab:\n",
    "#     print(word)\n",
    "\n",
    "# 300 차원의 가중치 벡터를 포함합니다.\n",
    "word2vec['it']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imdb 데이터셋에서 사용하는 단어의 가중치만 옮겨담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "num_words = 10000\n",
    "embedding_dim = 300\n",
    "\n",
    "# 케라스 데이터셋에서 word index를 제공해주고 있기 때문에,\n",
    "# 여기서는 word index를 만드는 작업을 생략하겠습니다.\n",
    "words_index = imdb.get_word_index()\n",
    "\n",
    "# embedding_matrix에 가중치를 옮겨 담습니다.\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# word: 단어, idx는 단어의 인덱스입니다.\n",
    "for word, idx in words_index.items():\n",
    "    if(idx < num_words):\n",
    "        if(word in word2vec):\n",
    "            word_weight = word2vec[word]\n",
    "        else:\n",
    "            word_weight = None\n",
    "\n",
    "        if word_weight is not None:\n",
    "            embedding_matrix[idx] = word_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imdb 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb 데이터셋을 준비합니다.\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 500\n",
    "\n",
    "pad_X_train = pad_sequences(X_train, maxlen=max_len, padding = 'pre')\n",
    "pad_X_test = pad_sequences(X_test, maxlen=max_len, padding = 'pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구성 및 Embedding Layer 가중치 전이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "# 이 층은 모델의 제일 첫 번째 층으로만 사용할 수 있습니다.\n",
    "# Flatten 층을 사용하기 위해 input_length를 전달합니다.\n",
    "model.add(Embedding(input_dim = num_words, output_dim = embedding_dim, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "# embedding_matrix에 담겨있는 가중치를 Embedding Layer로 옮기고,\n",
    "# 학습하지 않도록 동결시킵니다.\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지 word2vec을 사용하여,\n",
    "# 사전학습된 Embedding 층을 사용해보았습니다.\n",
    "history = model.fit(pad_X_train, y_train, \n",
    "                    batch_size = 32, \n",
    "                    epochs = 30, \n",
    "                    validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
