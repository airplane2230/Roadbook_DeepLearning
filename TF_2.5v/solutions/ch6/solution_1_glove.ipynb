{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Q1.</span>\n",
    "<b>우리는 IMDB 데이터셋에 Embedding층을 사용해보았습니다.  \n",
    "<span style=\"color:red\">Embedding층은 사전 학습된 가중치를 불러와 사용할 수 있습니다.</span>\n",
    "\n",
    "여러 차례 언급했듯이, 사전 학습된 가중치를 사용한다는 것은 모델 성능에 큰 향상을 가져올지도 모릅니다.  \n",
    "사전 학습된 가중치를 불러온 Embedding층을 활용하여 문제를 해결해보기를 바랍니다.  \n",
    "밑의 힌트에서 언급한 Glove 또는 Word2Vec 중 하나를 선택하여 진행하세요.</b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove 사용하기\n",
    "+ (저장소에선 파일을 제공하지 않습니다, 직접 다운받아주세요.) Glove 다운로드\n",
    "+ 링크는 책에서 제공하고 있습니다.  \n",
    "<br>\n",
    "+ 압축을 풀면 glove.6B.100d.txt파일이 보입니다.\n",
    "+ 100차원을 가지는 임베딩 가중치를 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### glove.txt에 존재하는 단어별 가중치를 옮겨담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# glove.6B.100d.txt 파일이 존재하는 경로를 입력해주세요.\n",
    "glove_dir = 'your_glove_file_path'\n",
    "\n",
    "embedding_dict = {}\n",
    "\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding = 'utf-8')\n",
    "\n",
    "# 가장 첫 번째 값은 'the', ','와 같은 단어이고,\n",
    "# 1:101은 해당 단어의 가중치를 나타냅니다.\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0] # 단어\n",
    "    weights = np.array(values[1:], dtype = 'float32') # 가중치\n",
    "    embedding_dict[word] = weights\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_dict) # 형태를 확인해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imdb 데이터셋에서 사용하는 단어의 가중치만 옮겨담기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "num_words = 10000\n",
    "embedding_dim = 100\n",
    "\n",
    "# 케라스 데이터셋에서 word index를 제공해주고 있기 때문에,\n",
    "# 여기서는 word index를 만드는 작업을 생략하겠습니다.\n",
    "words_index = imdb.get_word_index()\n",
    "\n",
    "# embedding_matrix에 가중치를 옮겨 담습니다.\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "# word: 단어, idx는 단어의 인덱스입니다.\n",
    "for word, idx in words_index.items():\n",
    "    if(idx < num_words): # 10,000번째 단어까지만 사용합니다.\n",
    "        word_weight = embedding_dict.get(word)\n",
    "        if word_weight is not None:\n",
    "            embedding_matrix[idx] = word_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### imdb 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imdb 데이터셋을 준비합니다.\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 500\n",
    "\n",
    "pad_X_train = pad_sequences(X_train, maxlen=max_len, padding = 'pre')\n",
    "pad_X_test = pad_sequences(X_test, maxlen=max_len, padding = 'pre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구성 및 Embedding Layer 가중치 전이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "# 이 층은 모델의 제일 첫 번째 층으로만 사용할 수 있습니다.\n",
    "# Flatten 층을 사용하기 위해 input_length를 전달합니다.\n",
    "model.add(Embedding(input_dim = num_words, output_dim = embedding_dim, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "# embedding_matrix에 담겨있는 가중치를 Embedding Layer로 옮기고,\n",
    "# 학습하지 않도록 동결시킵니다.\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기까지 Glove를 사용하여,\n",
    "# 사전학습된 Embedding 층을 사용해보았습니다.\n",
    "history = model.fit(pad_X_train, y_train, \n",
    "                    batch_size = 32, \n",
    "                    epochs = 30, \n",
    "                    validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_study",
   "language": "python",
   "name": "keras_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
