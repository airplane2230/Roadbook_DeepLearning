{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 데이터셋 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(777)\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 테스트셋은 사용하지 않기 때문에 생략합니다.\n",
    "(x_train, y_train), _ = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "# 채널을 추가합니다.(28, 28) -> (28, 28, 1)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, \n",
    "                                                  random_state = 777)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape} \\nx_val shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 객체를 생성합니다.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(1000).batch(32)\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 검증 데이터셋 객체를 생성합니다.\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape = (28, 28, 1))\n",
    "\n",
    "    x = Conv2D(32, 3, activation = 'relu')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 및 optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "val_loss = Mean(name='val_loss')\n",
    "val_accuracy = SparseCategoricalAccuracy(name='val_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 과정 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 스텝을 정의합니다.\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(images, training=True)\n",
    "        # 배치 학습 데이터의 개별 손실값을 구합니다.\n",
    "        loss = loss_object(labels, outputs)\n",
    "\n",
    "    # 손실값 참고하여, 그래디언트를 구합니다.\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # 모델의 가중치를 업데이트합니다.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # 학습 손실값을 계산합니다.\n",
    "    train_loss(loss)\n",
    "    # 학습 평가지표를 계산합니다.\n",
    "    train_accuracy(labels, outputs)\n",
    "\n",
    "# 검증 스텝을 정의합니다.\n",
    "@tf.function\n",
    "def val_step(images, labels):\n",
    "    outputs = model(images, training=False)\n",
    "    # 배치 검증 데이터의 개별 손실값을 구합니다.\n",
    "    v_loss = loss_object(labels, outputs)\n",
    "\n",
    "    # 검증 손실값을 계산합니다.\n",
    "    val_loss(v_loss)\n",
    "    # 검증 평가지표를 계산합니다.\n",
    "    val_accuracy(labels, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "# 수준: epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    # 다음 에폭을 위해 지표를 초기화합니다.\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "\n",
    "    # 수준: step\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    # 수준: step\n",
    "    for val_images, val_labels in val_ds:\n",
    "        val_step(val_images, val_labels)\n",
    "\n",
    "    print('Epoch: {}, train_loss: {}, train_acc: {} val_loss: {}, val_acc: {}'.format(\n",
    "          epoch + 1,\n",
    "          train_loss.result(), train_accuracy.result() * 100,\n",
    "          val_loss.result(), val_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr></hr>\n",
    "\n",
    "# TQDM 및 training setup 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 데이터셋 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(777)\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 테스트셋은 사용하지 않기 때문에 생략합니다.\n",
    "(x_train, y_train), _ = mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "# 채널을 추가합니다.(28, 28) -> (28, 28, 1)\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, \n",
    "                                                  random_state = 777)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape} \\nx_val shape: {x_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 객체를 생성합니다.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = train_ds.shuffle(1000).batch(32)\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 검증 데이터셋 객체를 생성합니다.\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input(shape = (28, 28, 1))\n",
    "\n",
    "    x = Conv2D(32, 3, activation = 'relu')(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation = 'relu')(x)\n",
    "    x = Dense(10, activation = 'softmax')(x)\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = x)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss 및 optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "loss_object = SparseCategoricalCrossentropy()\n",
    "optimizer = Adam(learning_rate = learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Mean\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "\n",
    "train_loss = Mean(name='train_loss')\n",
    "train_accuracy = SparseCategoricalAccuracy(name='train_accuracy')\n",
    "train_accuracy_mean = Mean(name = 'train_accuracy_mean')\n",
    "\n",
    "val_loss = Mean(name='val_loss')\n",
    "val_accuracy = SparseCategoricalAccuracy(name='val_accuracy')\n",
    "val_accuracy_mean = Mean(name = 'val_accuracy_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 과정 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 스텝을 정의합니다.\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(images, training=True)\n",
    "        # 배치 학습 데이터의 개별 손실값을 구합니다.\n",
    "        loss = loss_object(labels, outputs)\n",
    "    \n",
    "    # 손실값 참고하여, 그래디언트를 구합니다.\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    # 모델의 가중치를 업데이트합니다.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    # 학습 손실값을 계산합니다.\n",
    "    train_loss(loss)\n",
    "    # 학습 평가지표를 계산합니다.\n",
    "    train_accuracy(labels, outputs)\n",
    "    train_accuracy_mean(train_accuracy.result())\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# 검증 스텝을 정의합니다.\n",
    "@tf.function\n",
    "def val_step(images, labels):\n",
    "    outputs = model(images, training=False)\n",
    "    # 배치 검증 데이터의 개별 손실값을 구합니다.\n",
    "    v_loss = loss_object(labels, outputs)\n",
    "\n",
    "    # 검증 손실값을 계산합니다.\n",
    "    val_loss(v_loss)\n",
    "    # 검증 평가지표를 계산합니다.\n",
    "    val_accuracy(labels, outputs)\n",
    "    val_accuracy_mean(val_accuracy.result())\n",
    "    \n",
    "    return v_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot, val_loss_plot = [], []\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "# 수준: epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss, total_val_loss = 0, 0\n",
    "    \n",
    "    # 다음 에폭을 위해 지표를 초기화합니다.\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    train_accuracy_mean.reset_states()\n",
    "    \n",
    "    val_loss.reset_states()\n",
    "    val_accuracy.reset_states()\n",
    "    val_accuracy_mean.reset_states()\n",
    "    \n",
    "    tqdm_dataset = tqdm(enumerate(train_ds))\n",
    "    \n",
    "    # Train Step, 수준: step\n",
    "    for batch, (images, labels) in tqdm_dataset:\n",
    "        batch_loss = train_step(images, labels)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        tqdm_dataset.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Loss': '{:06f}'.format(batch_loss.numpy()),\n",
    "            'Total Loss': '{:06f}'.format(total_loss / (batch + 1)),\n",
    "            'Accracy': train_accuracy_mean.result().numpy()\n",
    "        })\n",
    "        \n",
    "        if batch % 30 == 0:\n",
    "            gc.collect()\n",
    "        \n",
    "    loss_plot.append(total_loss / (batch + 1))\n",
    "    \n",
    "    tqdm_dataset_val = tqdm(enumerate(val_ds))\n",
    "    \n",
    "    # Validation Step, 수준: step\n",
    "    for batch, (val_images, val_labels) in tqdm_dataset_val:\n",
    "        batch_val_loss = val_step(val_images, val_labels)\n",
    "        total_val_loss += batch_val_loss\n",
    "        \n",
    "        tqdm_dataset_val.set_postfix({\n",
    "            'Epoch': epoch + 1,\n",
    "            'Val Loss': '{:06f}'.format(batch_val_loss.numpy()),\n",
    "            'Val Total Loss': '{:06f}'.format(total_val_loss / (batch + 1)),\n",
    "            'Val Accracy': val_accuracy_mean.result().numpy()\n",
    "        })\n",
    "        \n",
    "    val_loss_plot.append(total_val_loss / (batch + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot, label = 'train_loss')\n",
    "plt.plot(val_loss_plot, label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
