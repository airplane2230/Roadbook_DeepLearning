{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\">Q1.</span>\n",
    "<b>MNIST 데이터셋은 복잡하지 않은 데이터셋이기 때문에 가벼운 환경에서도 다양한 실험을 해보기에 적합합니다.  \n",
    "필자는 계속해서 <span style=\"color:red\">신경망이 스케일에 매우 민감하다고 언급해왔습니다.</span>  \n",
    "MNIST 데이터셋에서의 스케일에 대한 전처리로 데이터를 255로 나누는 과정을 기억하나요?  \n",
    "이 과정을 거치지 않은 데이터셋에서의 결과와 비교해보기 바랍니다.<br>  \n",
    "또한, 보스턴 주택 가격 예측 문제에서도 스케일 문제를 해결하기 위해 표준화를 진행해주었습니다.  \n",
    "표준화를 적용하지 않은 상태에서 신경망을 학습시켜보고 결과를 비교해보길 바랍니다.</b>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "# 텐서플로우 저장소에서 데이터를 다운받습니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='mnist.npz')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/테스트 데이터를 0.7/0.3의 비율로 분리합니다.\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, \n",
    "                                                  random_state = 777)\n",
    "\n",
    "num_x_train = x_train.shape[0]\n",
    "num_x_val = x_val.shape[0]\n",
    "num_x_test = x_test.shape[0]\n",
    "\n",
    "# 모델의 입력으로 사용하기 위한 전처리 과정입니다.\n",
    "# 전처리를 진행하지 않습니다.\n",
    "x_train = (x_train.reshape((num_x_train, 28 * 28)))\n",
    "x_val = (x_val.reshape((num_x_val, 28 * 28)))\n",
    "x_test = (x_test.reshape((num_x_test, 28 * 28)))\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 각 데이터의 레이블을 범주형 형태로 변경합니다.\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 784차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense 층\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (784, )))\n",
    "model.add(Dense(32, activation = 'relu')) # 32개의 출력을 가지는 Dense 층\n",
    "model.add(Dense(10, activation = 'softmax')) # 10개의 출력을 가지는 신경망\n",
    "\n",
    "model.compile(optimizer='adam', # 옵티마이저 : Adam\n",
    "              loss = 'categorical_crossentropy', # 손실 함수 : categorical_crossentropy\n",
    "              metrics=['acc']) # 모니터링 할 평가지표 : acc\n",
    "\n",
    "# 작은 차이라고 느껴질 수 있지만, 분명히 전처리를 수행한\n",
    "# 데이터셋을 학습하는 것이 성능이 더 좋습니다.\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = 30, \n",
    "                    batch_size = 128, \n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.boston_housing import load_data\n",
    "\n",
    "# 데이터를 다운받습니다.\n",
    "(x_train, y_train), (x_test, y_test) = load_data(path='boston_housing.npz',\n",
    "                                                 test_split=0.2,\n",
    "                                                 seed=777)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 표준화\n",
    "# 전처리를 진행하지 않습니다.\n",
    "# mean = np.mean(x_train, axis = 0)\n",
    "# std = np.std(x_train, axis = 0)\n",
    "\n",
    "# x_train = (x_train - mean) / std\n",
    "# x_test = (x_test - mean) / std\n",
    "\n",
    "# 검증 데이터셋을 만듭니다.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.33, \n",
    "                                                  random_state = 777)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 입력 데이터의 형태를 꼭 명시해야 합니다.\n",
    "# 13차원의 데이터를 입력으로 받고, 64개의 출력을 가지는 첫 번째 Dense 층\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (13, )))\n",
    "model.add(Dense(32, activation = 'relu')) # 32개의 출력을 가지는 Dense 층\n",
    "model.add(Dense(1)) # 하나의 값을 출력합니다.\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])\n",
    "\n",
    "# 전처리한 코드의 결과와 비교했을때, 매우 큰 차이가 남을 볼 수 있습니다.\n",
    "# 또한, 학습이 진행되고 있는지 loss 값과 metrics를 확인하면서 점검해보세요.\n",
    "# 이번 문제를 통해 전처리 작업의 중요성(여기서는 스케일링)을 알 수 있습니다.\n",
    "history = model.fit(x_train, y_train, \n",
    "                    epochs = 300, \n",
    "                    validation_data = (x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_study",
   "language": "python",
   "name": "keras_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
